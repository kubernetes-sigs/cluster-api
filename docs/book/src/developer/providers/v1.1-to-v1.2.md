# Cluster API v1.1 compared to v1.2

This document provides an overview over relevant changes between ClusterAPI v1.1 and v1.2 for
maintainers of providers and consumers of our Go API.

## Minimum Kubernetes version for the management cluster

* The minimum Kubernetes version that can be used for a management cluster by Cluster API is now 1.20.0

## Minimum Go version

* The Go version used by Cluster API is now Go 1.17.x

## Dependencies

**Note**: Only the most relevant dependencies are listed, `k8s.io/` and `ginkgo`/`gomega` dependencies
in ClusterAPI are kept in sync with the versions used by `sigs.k8s.io/controller-runtime`.

- sigs.k8s.io/kind: v0.11.x => v0.14.x
- k8s.io/*: v0.23.x => v0.24.x (derived from controller-runtime)
- github.com/onsi/gomega: v0.17.0 => v0.18.1 (derived from controller-runtime)
- k8s.io/kubectl: v0.23.5 => 0.24.0

## Changes by Kind

### Deprecation

* `util.MachinesByCreationTimestamp` has been deprecated and will be removed in a future release.

### Removals
* The `third_party/kubernetes-drain` package has been removed, as we're now using `k8s.io/kubectl/pkg/drain` instead ([PR](https://github.com/kubernetes-sigs/cluster-api/pull/5440)).
* `util/version.CompareWithBuildIdentifiers` has been removed, please use `util/version.Compare(a, b, WithBuildTags())` instead.
* The functions `annotations.HasPausedAnnotation` and `annotations.HasSkipRemediationAnnotation` have been removed, please use
  `annotations.HasPaused` and `annotations.HasSkipRemediation` respectively instead.
* `ObjectMeta.ClusterName` has been removed from `k8s.io/apimachinery/pkg/apis/meta/v1`.

### API Changes

- `util.ClusterToInfrastructureMapFuncWithExternallyManagedCheck` was removed and the externally managed check was added to `util.ClusterToInfrastructureMapFunc`, which required changing its signature.
   Users of the former simply need to start using the latter and users of the latter need to add the new arguments to their call.

### Other

- Logging:
    - To align with the upstream Kubernetes community CAPI now configures logging via `component-base/logs`. 
      This provides advantages like support for the JSON logging format (via `--logging-format=json`) and automatic
      deprecation of klog flags aligned to the upstream Kubernetes deprecation period.
      <details>
      <summary>View <code>main.go</code> diff</summary>

      ```diff
      var (
      	...
      +	logOptions = logs.NewOptions()
      )

      func init() {
      -	klog.InitFlags(nil)

      func InitFlags(fs *pflag.FlagSet) {
      +	logs.AddFlags(fs, logs.SkipLoggingConfigurationFlags())
      +	logOptions.AddFlags(fs)
      
      func main() {
      	...
      	pflag.Parse()
      
      +	if err := logOptions.ValidateAndApply(); err != nil {
      +		setupLog.Error(err, "unable to start manager")
      +		os.Exit(1)
      +	}
      +
      +	// Set the Klog format, as the Serialize format shouldn't be used anymore.
      +	// This makes sure that the logs are formatted correctly, i.e.:
      +	// * JSON logging format: msg isn't serialized twice
      +	// * text logging format: values are formatted with their .String() func.
      +	ctrl.SetLogger(klogr.NewWithOptions(klogr.WithFormat(klogr.FormatKlog)))
      -	ctrl.SetLogger(klogr.New())
      ```
      </details>

      This change has been introduced in CAPI in the following PRs: [#6072](https://github.com/kubernetes-sigs/cluster-api/pull/6072), [#6190](https://github.com/kubernetes-sigs/cluster-api/pull/6190).</br>
      **Note**: This change is not mandatory for providers, but highly recommended.

- Following E2E framework functions are now checking that machines are created in the expected failure domain (if defined);
  all E2E tests can now verify failure domains too.
  - `ApplyClusterTemplateAndWait`
  - `WaitForControlPlaneAndMachinesReady`
  - `DiscoveryAndWaitForMachineDeployments`
- The `AssertControlPlaneFailureDomains` function in the E2E test framework has been modified to allow proper failure domain testing.

- After investigating an [issue](https://github.com/kubernetes-sigs/cluster-api/issues/6006) we discovered that improper implementation of a check on `cluster.status.infrastructureReady` can lead to problems during cluster deletion. As a consequence, we recommend that all providers ensure:
  - The check for `cluster.status.infrastructureReady=true` usually existing at the beginning of the reconcile loop for control-plane providers is implemented after setting external objects ref;
  - The check for `cluster.status.infrastructureReady=true` usually existing  at the beginning of the reconcile loop for infrastructure provider does not prevent the object to be deleted  
rif. [PR #6183](https://github.com/kubernetes-sigs/cluster-api/pull/6183)
